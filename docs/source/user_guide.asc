:numbered:

= UncleBench User Guide
CCN-HPC <dsp-cspit-ccn-hpc@edf.fr>
v1.0, 2017-01-27
:toc:
:data-uri:
:imagesdir: images

== What's UncleBench?

UncleBench is a benchmarking software currently running with Jube as a benchmarking environment "engine". 
Its architecture makes it easier to handle platforms settings, benchmark descriptions, sources and test cases as separate resource.
It provides useful commands to modify parameters on the fly without having to change the benchmark or platform description files.
Eventually a report command based on python plugins gives the possibility to build annotated HTML performance reports.

== Obtaining, building and installing UncleBench

UncleBench's code is available at GitHub https://github.com/edf-hpc/unclebench[https://github.com/edf-hpc/unclebench].
You can obtain a copy using *git*:

 $ git clone https://github.com/edf-hpc/unclebench.git


Debian packaging files are included, so if you are using a Debian or a Debian derivative system you only need to build a
Debian package and install it. Before building the package, you will need to install the following packages:

 $ apt-get install debhelper dh-python python-all python-setuptools pandoc dpkg-dev
	 
If you are not using Debian, UncleBench provides a setuptools script for its installation.
Just run:

 $ python setup.py install
     

==  Benchmark and platform files

Jube benchmark and platform description files are identified by UncleBench if they are located in the directories listed in the following table.

[cols="4*", options="header"]
|===
| Description
| Default directory
| Environment variable for custom path
| Local path

| Platform files location
| /usr/share/unclebenrch/platform 
| UBENCH_PLATFORM_DIR
| .config/unclebench/platform

| Benchmark files location
| /usr/share/unclebench/benchmarks
| UBENCH_BENCHMARK_DIR
| .config/unclebench/benchmarks
|===

=== Benchmark file writing

Benchmarks files are written using Jube XML format. The following constraints enables UncleBench integration:

A parameter named "nodes" must be used to define the number of nodes on which the benchmark should be run:
----
  <parameter name="nodes" type="int">1</parameter>
----

A parameter named "submit" must be used to launch the batch script:
----
  <do> $submit $submit_script </do>
----

A "platform" loading header must be added to enable the unclebench "-p" option:
----
  <include-path>
	<path> $UBENCH_PLATFORM_DIR </path>
  </include-path>
  <include from="platforms.xml" path="include-path"/>
----		
										   
=== Platform file writing

The platform directory should contain a "plaftorms.xml" file. This file lists all platforms that are made available to UncleBench.


[[app-listing]]
[source,xml]
.platforms.xml
----
  <?xml version="1.0" encoding="utf-8"?>
  <jube>

    <include-path>

    <path>$UBENCH_PLATFORM_DIR/</path>

    <path tag="platform1">$UBENCH_PLATFORM_DIR/Platform1</path>
    <path tag="platform2">$UBENCH_PLATFORM_DIR/Platform2</path>
  </jube>
----


Each platform directory (here Platform1 and Platform2) provides a main platform.xml Jube file defining all the parameters needed to define a platform.


== Download benchmark sources and test cases

=== multisource sections

A multisource section is needed in the <benchmark_name>.xml Jube file to describe where the sources and test cases can be found.

----
  <multisource>
        <source protocol="https">
	     <url>https://<server_url>/benchs/public</url>
	     <file>/stream/stream-5.10.tar.gz </file>
        </source>
  </multisource>
----


=== ubench fetch

 $ ubench fetch -b <benchmark_name>

This command downloads benchmark sources and test case files from a location specified by the *multisource* section to a local directory.
This default local directory where resource are fetched is */scratch/<user>/Ubench/resource* but can be customized with _UBENCH_RESOURCE_DIR_ environment variable.


== Run benchmark(s)

=== ubench run

The *ubench run* command launches one or multiple benchmarks on a given platform. The number of nodes or a node list with clustershell format can be .
It is also possible to customize benchmark parameters on the fly.

 $ ubench run -b <benchmark_name> -p <platform> [-w <nodes_list|number_of_nodes>] [-customp <paramter_name>:<value>]



Run <benchmark_name> on 4 nodes:

 $ ubench run -b <benchmark_name> -p <platform> -w 4
 --- Ubench platform name set to : <platform>
 ---- <benchmark_name> description files are already present in run directory and will be overwritten.
 ---- benchmark run directory : /scratch/<user>/Ubench/benchmarks/<platform>/<benchmark_name>/benchmark_runs/000000
 ---- Use the following command to follow benchmark progress: " ubench log -p <platform> -b <benchmark_name> -i 000000"


Run three times <benchmark_name> on 4,8 and 16 nodes:

  $ ubench run -b <benchmark_name> -p <platform> -w 4 8 16

Run <benchmark_name> on 6 nodes with precise given Ids:

  $ ubench run -p <platform> -b <benchmark_name> -w cn[100-105,205,207]


=== ubench list / ubench result

The *ubench list* command prints a table of all runs that have been launched for a benchmark on a given platform.

----
  $ ubench list -p <platform> -b <benchmark_name>
  
  Benchmark_name   | Platform   | ID     | Date              | Run_directory  | Nodes |
  ------------------------------------------------------------------------------------
  <benchmark_name> | <platform> | 000000 | Mon Jan 16 10...  | /scratch/....  | 1     |
  <benchmark_name> | <platform> | 000001 | Mon Jan 16 10...  | /scratch/....  | 4     | 
----

The *ubench result* command calls the *jube result* command that parse benchmark result files according to <analyse> and <result> xml sections content.
If no id is given results of the last executed benchmark are printed.


  $ ubench result -p <platform> -b <benchmark_name> [-i <benchmark_id]

Example with a hpcc benchmark launch with _-w 4 8 16_ option:

----
  $ ubench result -p <platform> -b hpcc

  Processing hpcc benchmark :
  ----analysing results
  ----extracting analysis
  nodes  MPIRandAcc_GUPs  MPIFFT_Gflops  PTRANS_GBs  StarDGEMM_Gflops  RORingBandwidth_GBytes  RORingLatency_usec  ...
  4      0.862475         106.63         33.4613     34.5633           0.496759                1.27526             ...          
  8      1.40598          177.489        70.8118     34.0936           0.401475                1.28486             ...
  16     2.62665          345.495        121.784     33.5744           0.348505                1.32338             ...
----

=== ubench status / ubench log

*ubench status* command prints the status of each step of a benchmark run.

----
  $ ubench status -p <platform> -b <benchmark_name>
  
  Status run dir: /scratch/<user>/Ubench/benchmarks/<platform>
  Processing <benchmark_name> benchmark :

    Status for step: compile
    --------------------------------
    id	started	done	workdir
    --------------------------------
    0 	true	true	/scratch/....

    Status for step: execute
    --------------------------------
    id	started	done	workdir
    --------------------------------
    1	true	false	/scratch/....
----

*ubench log* log prints the concatenation of every files in <analyse> section + standard files like stdout stderr and run.log.
It can be usefull to follow precisely the benchmark process without the need to dig in benchmark run directory.


== Advanced usage

=== Customize parameters on the fly

With UncleBench it is possible to customize benchmark parameters on the fly.
The *listparams* command lists every customizable parameter:

----
 $ ubench listparams -b hpl -p <platform_name>
 
 benchmark parameters
 -----------------------------------------------
      variant_v : 0
      variant_name : ["Full_MPI"][$variant_v]
      variant_NB : 192
      memory_proportion : 0.8
      variant_Ntemp : (${memory_proportion}*${nodes}*(${GB_per_node})*1e9/8) ** 0.5
      variant_N : int( round( ${variant_Ntemp} / ${variant_NB} ) * ${variant_NB})
      arch : ["gnu","intel"][$comp_v]
      nodes : 1
      taskspernode : $NUMA_regions*$cores_per_NUMA_region
      threadspertask : 1
      executable : ./xhpl
      modules : $module_compile $module_mpi $module_blas
      timelimit : 24:00:00
      args_starter : ${binding_full_node}
----

For example a HPL benchmark can be launched with _memory_proportion_ parameter customized to make HPL consume only 40 percent of available memory.

----
  $ubench run -b hpl -p <platform_name> -customp memory_proportion:0.4
  
  -- Ubench platform name set to : eocn
  ---- hpl description files are already present in run directory and will be overwritten.
  ---- memory_proportion parameter was modified from 0.8 to 0.4 for this run
  ---- benchmark run directory : /scratch/..../hpl/benchmark_runs/000000
  ---- Use the following command to follow benchmark progress :  " ubench log -p eocn -b hpl -i 000000"
----

=== Build performance report

The *ubench report* command generates asciidoctor source files that are compiled into a global performance report.
The report build of each benchmark is defined by python plugins that are found by default in "/usr/share/unclebench/lib/plugins/" directory,
this location can be customised with _UBENCH_PLUGIN_DIR_ environment variable. Additional details about reporting plugins can be found in the developer guide.

----
  $ubench report -b stream hpcc -p <platform_name>
  Do you want build a report section for stream benchmark(y/n)?  y
  /scratch/...
  Processing stream benchmark :
  ----analysing results
  ----extracting analysis
  ----building asciidoc file

  Do you want build a report section for hpcc benchmark(y/n)?  y
  /scratch/...
  Processing hpcc benchmark :
  ----analysing results
  ----extracting analysis
  ----building asciidoc file
  
  Report was built in /scratch/<user>/Ubench/benchmarks/reports/Report_2017-01-18-09:49
----

Asciidoctor files and a html version of the performance report produced by *ubench report* command can be found in the _Report\_<date>_ directory (c.f ).

image::perf_report_ex.png[perf_report_ex,role="center",title="Report example"]


